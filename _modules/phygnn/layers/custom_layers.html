

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>phygnn.layers.custom_layers &mdash; phygnn 0.1.dev1+g66d756c documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js?v=49f2134f"></script>
      <script src="../../../_static/doctools.js?v=888ff710"></script>
      <script src="../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            phygnn
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Home page</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../misc/installation_usage.html">Installation and Usage</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../misc/installation.html">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../misc/installation.html#simple-install">Simple Install</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../misc/installation.html#developer-install">Developer Install</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../_autosummary/phygnn.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../_autosummary/phygnn.base.html">phygnn.base</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/phygnn.base.CustomNetwork.html">phygnn.base.CustomNetwork</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.base.CustomNetwork.html#phygnn.base.CustomNetwork"><code class="docutils literal notranslate"><span class="pre">CustomNetwork</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/phygnn.base.GradientUtils.html">phygnn.base.GradientUtils</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.base.GradientUtils.html#phygnn.base.GradientUtils"><code class="docutils literal notranslate"><span class="pre">GradientUtils</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../_autosummary/phygnn.layers.html">phygnn.layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.html">phygnn.layers.custom_layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.Attention.html">phygnn.layers.custom_layers.Attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.AxialAttentionBlock.html">phygnn.layers.custom_layers.AxialAttentionBlock</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.CBAM.html">phygnn.layers.custom_layers.CBAM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.ExpandDims.html">phygnn.layers.custom_layers.ExpandDims</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.FNO.html">phygnn.layers.custom_layers.FNO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.FlattenAxis.html">phygnn.layers.custom_layers.FlattenAxis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.FlexiblePadding.html">phygnn.layers.custom_layers.FlexiblePadding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.FunctionalLayer.html">phygnn.layers.custom_layers.FunctionalLayer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.GaussianAveragePooling2D.html">phygnn.layers.custom_layers.GaussianAveragePooling2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.GaussianNoiseAxis.html">phygnn.layers.custom_layers.GaussianNoiseAxis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.LogTransform.html">phygnn.layers.custom_layers.LogTransform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.MaskedSqueezeAndExcitation.html">phygnn.layers.custom_layers.MaskedSqueezeAndExcitation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.SigLin.html">phygnn.layers.custom_layers.SigLin</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.SkipConnection.html">phygnn.layers.custom_layers.SkipConnection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.SparseAttention.html">phygnn.layers.custom_layers.SparseAttention</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.SpatialExpansion.html">phygnn.layers.custom_layers.SpatialExpansion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.SpatioTemporalExpansion.html">phygnn.layers.custom_layers.SpatioTemporalExpansion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.SqueezeAndExcitation.html">phygnn.layers.custom_layers.SqueezeAndExcitation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.Sup3rAdder.html">phygnn.layers.custom_layers.Sup3rAdder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.Sup3rConcat.html">phygnn.layers.custom_layers.Sup3rConcat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.Sup3rConcatObs.html">phygnn.layers.custom_layers.Sup3rConcatObs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.Sup3rObsModel.html">phygnn.layers.custom_layers.Sup3rObsModel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.TileLayer.html">phygnn.layers.custom_layers.TileLayer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.custom_layers.UnitConversion.html">phygnn.layers.custom_layers.UnitConversion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/phygnn.layers.handlers.html">phygnn.layers.handlers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.handlers.HiddenLayers.html">phygnn.layers.handlers.HiddenLayers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.layers.handlers.Layers.html">phygnn.layers.handlers.Layers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../_autosummary/phygnn.model_interfaces.html">phygnn.model_interfaces</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/phygnn.model_interfaces.base_model.html">phygnn.model_interfaces.base_model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.model_interfaces.base_model.ModelBase.html">phygnn.model_interfaces.base_model.ModelBase</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/phygnn.model_interfaces.phygnn_model.html">phygnn.model_interfaces.phygnn_model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.model_interfaces.phygnn_model.PhygnnModel.html">phygnn.model_interfaces.phygnn_model.PhygnnModel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/phygnn.model_interfaces.random_forest_model.html">phygnn.model_interfaces.random_forest_model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.model_interfaces.random_forest_model.RandomForestModel.html">phygnn.model_interfaces.random_forest_model.RandomForestModel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/phygnn.model_interfaces.tf_model.html">phygnn.model_interfaces.tf_model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.model_interfaces.tf_model.TfModel.html">phygnn.model_interfaces.tf_model.TfModel</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../_autosummary/phygnn.phygnn.html">phygnn.phygnn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/phygnn.phygnn.PhysicsGuidedNeuralNetwork.html">phygnn.phygnn.PhysicsGuidedNeuralNetwork</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.phygnn.PhysicsGuidedNeuralNetwork.html#phygnn.phygnn.PhysicsGuidedNeuralNetwork"><code class="docutils literal notranslate"><span class="pre">PhysicsGuidedNeuralNetwork</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.html">phygnn.utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.loss_metrics.html">phygnn.utilities.loss_metrics</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.loss_metrics.binary_crossentropy.html">phygnn.utilities.loss_metrics.binary_crossentropy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.loss_metrics.mae.html">phygnn.utilities.loss_metrics.mae</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.loss_metrics.mbe.html">phygnn.utilities.loss_metrics.mbe</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.loss_metrics.mse.html">phygnn.utilities.loss_metrics.mse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.loss_metrics.relative_mae.html">phygnn.utilities.loss_metrics.relative_mae</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.loss_metrics.relative_mbe.html">phygnn.utilities.loss_metrics.relative_mbe</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.loss_metrics.relative_mse.html">phygnn.utilities.loss_metrics.relative_mse</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.pre_processing.html">phygnn.utilities.pre_processing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.pre_processing.PreProcess.html">phygnn.utilities.pre_processing.PreProcess</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.tf_utilities.html">phygnn.utilities.tf_utilities</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.tf_utilities.idw_fill.html">phygnn.utilities.tf_utilities.idw_fill</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.tf_utilities.mean_fill.html">phygnn.utilities.tf_utilities.mean_fill</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.tf_utilities.tf_isin.html">phygnn.utilities.tf_utilities.tf_isin</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../_autosummary/phygnn.utilities.tf_utilities.tf_log10.html">phygnn.utilities.tf_utilities.tf_log10</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">phygnn</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">phygnn.layers.custom_layers</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for phygnn.layers.custom_layers</h1><div class="highlight"><pre>
<span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="sd">&quot;&quot;&quot;Custom tf layers.&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">phygnn.utilities.tf_utilities</span><span class="w"> </span><span class="kn">import</span> <span class="n">idw_fill</span><span class="p">,</span> <span class="n">mean_fill</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="FlexiblePadding"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.FlexiblePadding.html#phygnn.layers.custom_layers.FlexiblePadding">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">FlexiblePadding</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Class to perform padding on tensors&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;REFLECT&#39;</span><span class="p">,</span> <span class="n">option</span><span class="o">=</span><span class="s1">&#39;tf&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        paddings : int array</span>
<span class="sd">            Integer array with shape [n,2] where n is the</span>
<span class="sd">            rank of the tensor and elements give the number</span>
<span class="sd">            of leading and trailing pads</span>
<span class="sd">        mode : str</span>
<span class="sd">            tf.pad() / np.pad() padding mode. Can be REFLECT, CONSTANT,</span>
<span class="sd">            or SYMMETRIC</span>
<span class="sd">        option : str</span>
<span class="sd">            Option for TensorFlow padding (&quot;tf&quot;) or numpy (&quot;np&quot;). Default is tf</span>
<span class="sd">            for tensorflow training. We have observed silent failures of</span>
<span class="sd">            tf.pad() with larger array sizes, so &quot;np&quot; might be preferable at</span>
<span class="sd">            inference time on large chunks, but it is much slower when it has</span>
<span class="sd">            to convert tensors to numpy arrays. See the tensorflow issue</span>
<span class="sd">            https://github.com/tensorflow/tensorflow/issues/91027</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">option</span> <span class="o">=</span> <span class="n">option</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">option</span> <span class="o">==</span> <span class="s1">&#39;tf&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pad_fun</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">option</span> <span class="o">==</span> <span class="s1">&#39;np&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pad_fun</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s1">&#39;FlexiblePadding option must be &quot;tf&quot; or &quot;np&quot; but &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;received: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">option</span><span class="si">}</span><span class="s1">&#39;</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

<div class="viewcode-block" id="FlexiblePadding.compute_output_shape"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.FlexiblePadding.html#phygnn.layers.custom_layers.FlexiblePadding.compute_output_shape">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes output shape after padding</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            shape of input tensor</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        output_shape : tf.TensorShape</span>
<span class="sd">            shape of padded tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">):</span>
            <span class="n">output_shape</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">paddings</span><span class="p">[</span><span class="n">d</span><span class="p">])</span> <span class="o">+</span> <span class="n">input_shape</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="FlexiblePadding.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.FlexiblePadding.html#phygnn.layers.custom_layers.FlexiblePadding.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calls the padding routine</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            tensor on which to perform padding</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            padded tensor with shape given</span>
<span class="sd">            by compute_output_shape</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_fun</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">paddings</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ExpandDims"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.ExpandDims.html#phygnn.layers.custom_layers.ExpandDims">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">ExpandDims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Layer to add an extra dimension to a tensor.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int</span>
<span class="sd">            Target axis at which to expand the shape of the input. Default is</span>
<span class="sd">            axis 3 based on creating a new temporal axis of the default</span>
<span class="sd">            spatiotemporal shape of: (n_observations, n_spatial_0, n_spatial_1,</span>
<span class="sd">            n_temporal, n_features)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_axis</span> <span class="o">=</span> <span class="n">axis</span>

<div class="viewcode-block" id="ExpandDims.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.ExpandDims.html#phygnn.layers.custom_layers.ExpandDims.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calls the expand dims operation</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor with an extra dimension based on the init axes arg</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_axis</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="TileLayer"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.TileLayer.html#phygnn.layers.custom_layers.TileLayer">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">TileLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Layer to tile (repeat) data across a given axis.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">multiples</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        multiples : list</span>
<span class="sd">            This is a list with the same length as number of dimensions in the</span>
<span class="sd">            input tensor. Each entry in the list determines how many times to</span>
<span class="sd">            tile each axis in the tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mult</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">multiples</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<div class="viewcode-block" id="TileLayer.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.TileLayer.html#phygnn.layers.custom_layers.TileLayer.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calls the tile operation</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor with the specified axes tiled into larger shapes</span>
<span class="sd">            based on the multiples initialization argument.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mult</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="GaussianAveragePooling2D"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.GaussianAveragePooling2D.html#phygnn.layers.custom_layers.GaussianAveragePooling2D">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">GaussianAveragePooling2D</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Custom layer to implement tensorflow average pooling layer but with a</span>
<span class="sd">    gaussian kernel. This is basically a gaussian smoothing layer with a fixed</span>
<span class="sd">    convolution window that limits the area of effect&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pool_size</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        pool_size: integer</span>
<span class="sd">            Pooling window size. This sets the number of pixels in each</span>
<span class="sd">            dimension that will be averaged into an output pixel. Only one</span>
<span class="sd">            integer is specified, the same window length will be used for both</span>
<span class="sd">            dimensions. For example, if ``pool_size=2`` and ``strides=2`` then</span>
<span class="sd">            the output dimension will be half of the input.</span>
<span class="sd">        strides: Integer, tuple of 2 integers, or None.</span>
<span class="sd">            Strides values. If None, it will default to `pool_size`.</span>
<span class="sd">        padding: One of `&quot;valid&quot;` or `&quot;same&quot;` (case-insensitive).</span>
<span class="sd">            `&quot;valid&quot;` means no padding. `&quot;same&quot;` results in padding evenly to</span>
<span class="sd">            the left/right or up/down of the input such that output has the</span>
<span class="sd">            same height/width dimension as the input.</span>
<span class="sd">        sigma : float</span>
<span class="sd">            Sigma parameter for gaussian distribution</span>
<span class="sd">        trainable : bool</span>
<span class="sd">            Flag for whether sigma is trainable weight or not.</span>
<span class="sd">        kwargs : dict</span>
<span class="sd">            Extra kwargs for tf.keras.layers.Layer</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pool_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">),</span> <span class="s1">&#39;pool_size must be int!&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span> <span class="o">=</span> <span class="n">pool_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">strides</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="n">trainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

    <span class="c1"># pylint: disable=unused-argument</span>
<div class="viewcode-block" id="GaussianAveragePooling2D.build"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.GaussianAveragePooling2D.html#phygnn.layers.custom_layers.GaussianAveragePooling2D.build">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Custom implementation of the tf layer build method.</span>

<span class="sd">        Initializes the trainable sigma variable</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">):</span>
            <span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">trainable</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
            <span class="p">)</span></div>

<div class="viewcode-block" id="GaussianAveragePooling2D.make_kernel"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.GaussianAveragePooling2D.html#phygnn.layers.custom_layers.GaussianAveragePooling2D.make_kernel">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">make_kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates 2D gaussian kernel with side length `self.pool_size` and a</span>
<span class="sd">        sigma of `sigma`</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        kernel : np.ndarray</span>
<span class="sd">            2D kernel with shape (self.pool_size, self.pool_size)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
            <span class="o">-</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">,</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">gauss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
            <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">gauss</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">gauss</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">kernel</span></div>

<div class="viewcode-block" id="GaussianAveragePooling2D.get_config"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.GaussianAveragePooling2D.html#phygnn.layers.custom_layers.GaussianAveragePooling2D.get_config">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Implementation of get_config method from tf.keras.layers.Layer for</span>
<span class="sd">        saving/loading as part of keras sequential model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        config : dict</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
            <span class="s1">&#39;pool_size&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span><span class="p">,</span>
            <span class="s1">&#39;strides&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span>
            <span class="s1">&#39;padding&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="s1">&#39;trainable&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
            <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">),</span>
        <span class="p">})</span>
        <span class="k">return</span> <span class="n">config</span></div>

<div class="viewcode-block" id="GaussianAveragePooling2D.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.GaussianAveragePooling2D.html#phygnn.layers.custom_layers.GaussianAveragePooling2D.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Operates on x with the specified function</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor operated on by the specified function</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_kernel</span><span class="p">()</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idf</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">fslice</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">idf</span><span class="p">,</span> <span class="n">idf</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">iout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span>
                <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">fslice</span><span class="p">],</span>
                <span class="n">kernel</span><span class="p">,</span>
                <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">iout</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;concat&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span></div></div>


<div class="viewcode-block" id="GaussianNoiseAxis"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.GaussianNoiseAxis.html#phygnn.layers.custom_layers.GaussianNoiseAxis">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">GaussianNoiseAxis</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Layer to apply random noise along a given axis.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int | list | tuple</span>
<span class="sd">            Axes to apply random noise across. All other axes will have the</span>
<span class="sd">            same noise. For example, for a 5D spatiotemporal tensor with</span>
<span class="sd">            axis=(1, 2, 3) (both spatial axes and the temporal axis), this</span>
<span class="sd">            layer will apply a single random number to every unique index of</span>
<span class="sd">            axis=(1, 2, 3).</span>
<span class="sd">        mean : float</span>
<span class="sd">            The mean of the normal distribution.</span>
<span class="sd">        stddev : float</span>
<span class="sd">            The standard deviation of the normal distribution.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="k">else</span> <span class="p">[</span><span class="n">axis</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stddev</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">stddev</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_rand_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get shape of random noise along the specified axes.&quot;&quot;&quot;</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_axis</span><span class="p">:</span>
            <span class="n">shape</span><span class="p">[</span><span class="n">ax</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">ax</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<div class="viewcode-block" id="GaussianNoiseAxis.build"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.GaussianNoiseAxis.html#phygnn.layers.custom_layers.GaussianNoiseAxis.build">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Custom implementation of the tf layer build method.</span>

<span class="sd">        Sets the shape of the random noise along the specified axis</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="GaussianNoiseAxis.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.GaussianNoiseAxis.html#phygnn.layers.custom_layers.GaussianNoiseAxis.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calls the tile operation</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor with noise applied to the requested axis.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">rand_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_rand_shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="n">mean</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_mean</span><span class="p">,</span>
            <span class="n">stddev</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_stddev</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">rand_tensor</span></div></div>


<div class="viewcode-block" id="FlattenAxis"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.FlattenAxis.html#phygnn.layers.custom_layers.FlattenAxis">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">FlattenAxis</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Layer to flatten an axis from a 5D spatiotemporal Tensor into axis-0</span>
<span class="sd">    observations.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis : int</span>
<span class="sd">            Target axis that holds the dimension to be flattened into the</span>
<span class="sd">            axis-0 dimension. Default is axis 3 based on flatteneing the</span>
<span class="sd">            temporal axis of the default spatiotemporal shape of:</span>
<span class="sd">            (n_observations, n_spatial_0, n_spatial_1, n_temporal, n_features)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_axis</span> <span class="o">=</span> <span class="n">axis</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_check_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Assert that the shape of the input tensor is the expected 5D</span>
<span class="sd">        spatiotemporal shape</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s1">&#39;Input to FlattenAxis must be 5D with dimensions: &#39;</span>
            <span class="s1">&#39;(n_observations, n_spatial_0, n_spatial_1, n_temporal, &#39;</span>
            <span class="s1">&#39;n_features), but received shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">,</span> <span class="n">msg</span>

<div class="viewcode-block" id="FlattenAxis.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.FlattenAxis.html#phygnn.layers.custom_layers.FlattenAxis.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calls the flatten axis operation</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            5D spatiotemporal tensor with dimensions:</span>
<span class="sd">            (n_observations, n_spatial_0, n_spatial_1, n_temporal, n_features)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            4D spatiotemporal tensor with target axis flattened into axis 0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_shape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_axis</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="SpatialExpansion"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SpatialExpansion.html#phygnn.layers.custom_layers.SpatialExpansion">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">SpatialExpansion</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Class to expand the spatial dimensions of tensors with shape:</span>
<span class="sd">    (n_observations, n_spatial_0, n_spatial_1, n_features)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spatial_mult</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">spatial_method</span><span class="o">=</span><span class="s1">&#39;depth_to_space&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        spatial_mult : int</span>
<span class="sd">            Number of times to multiply the spatial dimensions. Note that the</span>
<span class="sd">            spatial expansion is an un-packing of the feature dimension. For</span>
<span class="sd">            example, if the input layer has shape (123, 5, 5, 16) with</span>
<span class="sd">            multiplier=2 the output shape will be (123, 10, 10, 4). The</span>
<span class="sd">            input feature dimension must be divisible by the spatial multiplier</span>
<span class="sd">            squared.</span>
<span class="sd">        spatial_method : str</span>
<span class="sd">            Either &quot;depth_to_space&quot; or an interpolation method for</span>
<span class="sd">            tf.image.resize().</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">spatial_mult</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_meth</span> <span class="o">=</span> <span class="n">spatial_method</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_check_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Assert that the shape of the input tensor is the expected 4D</span>
<span class="sd">        spatiotemporal shape</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s1">&#39;Input to SpatialExpansion must be 4D with dimensions: &#39;</span>
            <span class="s1">&#39;(n_observations, n_spatial_0, n_spatial_1, n_features), &#39;</span>
            <span class="s1">&#39;but received shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="n">msg</span>

<div class="viewcode-block" id="SpatialExpansion.build"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SpatialExpansion.html#phygnn.layers.custom_layers.SpatialExpansion.build">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Custom implementation of the tf layer build method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_spatial_expand</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Expand the two spatial dimensions (axis=1,2) of a 4D tensor using</span>
<span class="sd">        data from the last axes&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_meth</span> <span class="o">==</span> <span class="s1">&#39;depth_to_space&#39;</span><span class="p">:</span>
            <span class="n">check_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span><span class="o">**</span><span class="mi">2</span>
            <span class="k">if</span> <span class="n">check_shape</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="s1">&#39;Spatial expansion of factor </span><span class="si">{}</span><span class="s1"> is being attempted on &#39;</span>
                    <span class="s1">&#39;input tensor of shape </span><span class="si">{}</span><span class="s1">, but the last dimension of the &#39;</span>
                    <span class="s1">&#39;input tensor (</span><span class="si">{}</span><span class="s1">) must be divisible by the spatial &#39;</span>
                    <span class="s1">&#39;factor squared (</span><span class="si">{}</span><span class="s1">).&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span><span class="p">,</span>
                        <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                        <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

            <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">depth_to_space</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">s_expand_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
                <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span><span class="p">,</span>
                <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span><span class="p">,</span>
            <span class="p">])</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s_expand_shape</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_spatial_meth</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>

<div class="viewcode-block" id="SpatialExpansion.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SpatialExpansion.html#phygnn.layers.custom_layers.SpatialExpansion.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call the custom SpatialExpansion layer</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            4D spatial tensor</span>
<span class="sd">            (n_observations, n_spatial_0, n_spatial_1, n_features)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            4D spatiotemporal tensor with axes 1,2 expanded (if spatial_mult&gt;1)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_shape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_expand</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="SpatioTemporalExpansion"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SpatioTemporalExpansion.html#phygnn.layers.custom_layers.SpatioTemporalExpansion">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">SpatioTemporalExpansion</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Class to expand the spatiotemporal dimensions of tensors with shape:</span>
<span class="sd">    (n_observations, n_spatial_0, n_spatial_1, n_temporal, n_features)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">spatial_mult</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">temporal_mult</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">spatial_method</span><span class="o">=</span><span class="s1">&#39;depth_to_space&#39;</span><span class="p">,</span>
        <span class="n">temporal_method</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span>
        <span class="n">t_roll</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        spatial_mult : int</span>
<span class="sd">            Number of times to multiply the spatial dimensions. Note that the</span>
<span class="sd">            spatial expansion is an un-packing of the feature dimension. For</span>
<span class="sd">            example, if the input layer has shape (123, 5, 5, 24, 16) with</span>
<span class="sd">            multiplier=2 the output shape will be (123, 10, 10, 24, 4). The</span>
<span class="sd">            input feature dimension must be divisible by the spatial multiplier</span>
<span class="sd">            squared.</span>
<span class="sd">        temporal_mult : int</span>
<span class="sd">            Number of times to multiply the temporal dimension. For example,</span>
<span class="sd">            if the input layer has shape (123, 5, 5, 24, 2) with multiplier=2</span>
<span class="sd">            the output shape will be (123, 5, 5, 48, 2).</span>
<span class="sd">        spatial_method : str</span>
<span class="sd">            Either &quot;depth_to_space&quot; or an interpolation method for</span>
<span class="sd">            tf.image.resize().</span>
<span class="sd">        temporal_method : str</span>
<span class="sd">            Interpolation method for tf.image.resize(). Can also be</span>
<span class="sd">            &quot;depth_to_time&quot; for an operation similar to tf.nn.depth_to_space</span>
<span class="sd">            where the feature axis is unpacked into the temporal axis.</span>
<span class="sd">        t_roll : int</span>
<span class="sd">            Option to roll the temporal axis after expanding. When using</span>
<span class="sd">            temporal_method=&quot;depth_to_time&quot;, the default (t_roll=0) will add</span>
<span class="sd">            temporal steps after the input steps such that if input temporal</span>
<span class="sd">            shape is 3 and the temporal_mult is 24x, the output will have the</span>
<span class="sd">            index-0 timesteps at idt=0,24,48 but if t_roll=12, the output will</span>
<span class="sd">            have the original timesteps at idt=12,36,60. This is no longer</span>
<span class="sd">            recommended, as a positive roll will move the features of timestep</span>
<span class="sd">            -1 from the end of the series to the beginning.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">spatial_mult</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_temporal_mult</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">temporal_mult</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_temporal_meth</span> <span class="o">=</span> <span class="n">temporal_method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_meth</span> <span class="o">=</span> <span class="n">spatial_method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_t_roll</span> <span class="o">=</span> <span class="n">t_roll</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_check_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Assert that the shape of the input tensor is the expected 5D</span>
<span class="sd">        spatiotemporal shape</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s1">&#39;Input to SpatioTemporalExpansion must be 5D with dimensions: &#39;</span>
            <span class="s1">&#39;(n_observations, n_spatial_0, n_spatial_1, n_temporal, &#39;</span>
            <span class="s1">&#39;n_features), but received shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">,</span> <span class="n">msg</span>

<div class="viewcode-block" id="SpatioTemporalExpansion.build"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SpatioTemporalExpansion.html#phygnn.layers.custom_layers.SpatioTemporalExpansion.build">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Custom implementation of the tf layer build method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_temporal_expand</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Expand the temporal dimension (axis=3) of a 5D tensor&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temporal_meth</span> <span class="o">==</span> <span class="s1">&#39;depth_to_time&#39;</span><span class="p">:</span>
            <span class="n">check_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temporal_mult</span>
            <span class="k">if</span> <span class="n">check_shape</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="s1">&#39;Temporal expansion of factor </span><span class="si">{}</span><span class="s1"> is being attempted on &#39;</span>
                    <span class="s1">&#39;input tensor of shape </span><span class="si">{}</span><span class="s1">, but the last dimension of &#39;</span>
                    <span class="s1">&#39;the input tensor (</span><span class="si">{}</span><span class="s1">) must be divisible by the &#39;</span>
                    <span class="s1">&#39;temporal factor (</span><span class="si">{}</span><span class="s1">).&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_temporal_mult</span><span class="p">,</span>
                        <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                        <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_temporal_mult</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

            <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temporal_mult</span><span class="p">,</span>
                <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temporal_mult</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_t_roll</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">t_expand_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
                <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temporal_mult</span><span class="p">,</span>
            <span class="p">])</span>
            <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">x_unstack</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
                <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span>
                        <span class="n">x_unstack</span><span class="p">,</span>
                        <span class="n">t_expand_shape</span><span class="p">,</span>
                        <span class="n">method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_temporal_meth</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_spatial_expand</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Expand the two spatial dimensions (axis=1,2) of a 5D tensor using</span>
<span class="sd">        data from the last axes&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_meth</span> <span class="o">==</span> <span class="s1">&#39;depth_to_space&#39;</span><span class="p">:</span>
            <span class="n">check_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span><span class="o">**</span><span class="mi">2</span>
            <span class="k">if</span> <span class="n">check_shape</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="s1">&#39;Spatial expansion of factor </span><span class="si">{}</span><span class="s1"> is being attempted on &#39;</span>
                    <span class="s1">&#39;input tensor of shape </span><span class="si">{}</span><span class="s1">, but the last dimension of the &#39;</span>
                    <span class="s1">&#39;input tensor (</span><span class="si">{}</span><span class="s1">) must be divisible by the spatial &#39;</span>
                    <span class="s1">&#39;factor squared (</span><span class="si">{}</span><span class="s1">).&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span><span class="p">,</span>
                        <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                        <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

            <span class="n">out</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">depth_to_space</span><span class="p">(</span><span class="n">x_unstack</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span><span class="p">)</span>
                   <span class="k">for</span> <span class="n">x_unstack</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">s_expand_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
                <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span><span class="p">,</span>
                <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span><span class="p">,</span>
            <span class="p">])</span>
            <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">x_unstack</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
                <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span>
                        <span class="n">x_unstack</span><span class="p">,</span>
                        <span class="n">s_expand_shape</span><span class="p">,</span>
                        <span class="n">method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_spatial_meth</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<div class="viewcode-block" id="SpatioTemporalExpansion.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SpatioTemporalExpansion.html#phygnn.layers.custom_layers.SpatioTemporalExpansion.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call the custom SpatioTemporalExpansion layer</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            5D spatiotemporal tensor.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            5D spatiotemporal tensor with axes 1,2 expanded (if spatial_mult&gt;1)</span>
<span class="sd">            and axes 3 expanded (if temporal_mult&gt;1).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_shape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temporal_mult</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temporal_expand</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_mult</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spatial_expand</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="SkipConnection"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SkipConnection.html#phygnn.layers.custom_layers.SkipConnection">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">SkipConnection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Custom layer to implement a skip connection. This layer should be</span>
<span class="sd">    initialized and referenced in a layer list by the same name as both the</span>
<span class="sd">    skip start and skip end.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;add&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str</span>
<span class="sd">            Unique string identifier of the skip connection. The skip endpoint</span>
<span class="sd">            should have the same name.</span>
<span class="sd">        method : str</span>
<span class="sd">            Method to use for combining the skip start data and skip end data.</span>
<span class="sd">            Defaults to &#39;add&#39;. If &#39;concat&#39; this is applied along the trailing</span>
<span class="sd">            axis</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_method</span> <span class="o">=</span> <span class="n">method</span>

<div class="viewcode-block" id="SkipConnection.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SkipConnection.html#phygnn.layers.custom_layers.SkipConnection.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call the custom SkipConnection layer</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor. If this is the skip start, the input will be cached</span>
<span class="sd">            and returned without manipulation. If this is the skip endpoint,</span>
<span class="sd">            the output will be the input x combined with the tensor cached at</span>
<span class="sd">            the skip start. The tensors will be combined according to the</span>
<span class="sd">            method given at initialization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="n">x</span>
            <span class="k">return</span> <span class="n">x</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_method</span> <span class="o">==</span> <span class="s1">&#39;concat&#39;</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">tf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_method</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s1">&#39;Could not </span><span class="si">{}</span><span class="s1"> SkipConnection &quot;</span><span class="si">{}</span><span class="s1">&quot; data cache of &#39;</span>
                <span class="s1">&#39;shape </span><span class="si">{}</span><span class="s1"> to input of shape </span><span class="si">{}</span><span class="s1">.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_method</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">return</span> <span class="n">out</span></div></div>


<div class="viewcode-block" id="SqueezeAndExcitation"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SqueezeAndExcitation.html#phygnn.layers.custom_layers.SqueezeAndExcitation">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">SqueezeAndExcitation</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Custom layer for squeeze and excitation block for convolutional networks</span>

<span class="sd">    Note that this is only set up to take a channels-last conv output</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    1. Hu, Jie, et al. Squeeze-and-Excitation Networks. arXiv:1709.01507,</span>
<span class="sd">       arXiv, 16 May 2019, http://arxiv.org/abs/1709.01507.</span>
<span class="sd">    2. Pröve, Paul-Louis. “Squeeze-and-Excitation Networks.” Medium, 18 Oct.</span>
<span class="sd">       2017,</span>
<span class="sd">    https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ratio</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ratio : int</span>
<span class="sd">            Number of convolutional channels/filters divided by the number of</span>
<span class="sd">            dense connections in the SE block.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ratio</span> <span class="o">=</span> <span class="n">ratio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dense_units</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layers</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="SqueezeAndExcitation.build"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SqueezeAndExcitation.html#phygnn.layers.custom_layers.SqueezeAndExcitation.build">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build the SqueezeAndExcitation layer based on an input shape</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dense_units</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ratio</span><span class="p">))</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">pool_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling2D</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="n">pool_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling3D</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s1">&#39;SqueezeAndExcitation layer can only accept 4D or 5D data &#39;</span>
                <span class="s1">&#39;for image or video input but received input shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">input_shape</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">pool_layer</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dense_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Multiply</span><span class="p">(),</span>
        <span class="p">]</span></div>

<div class="viewcode-block" id="SqueezeAndExcitation.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SqueezeAndExcitation.html#phygnn.layers.custom_layers.SqueezeAndExcitation.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call the custom SqueezeAndExcitation layer</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor, this is the squeeze-and-excitation weights</span>
<span class="sd">            multiplied by the original input tensor x</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">t_in</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># multiply layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]([</span><span class="n">t_in</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="MaskedSqueezeAndExcitation"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.MaskedSqueezeAndExcitation.html#phygnn.layers.custom_layers.MaskedSqueezeAndExcitation">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">MaskedSqueezeAndExcitation</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Custom layer for masked squeeze and excitation block for convolutional</span>
<span class="sd">    networks</span>

<span class="sd">    Note that this is only set up to take a channels-last conv output&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ratio</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ratio : int</span>
<span class="sd">            Number of convolutional channels/filters divided by the number of</span>
<span class="sd">            dense connections in the SE block.</span>
<span class="sd">        name : str</span>
<span class="sd">            Name of layer</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ratio</span> <span class="o">=</span> <span class="n">ratio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dense_units</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layers</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="MaskedSqueezeAndExcitation.build"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.MaskedSqueezeAndExcitation.html#phygnn.layers.custom_layers.MaskedSqueezeAndExcitation.build">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build the SqueezeAndExcitation layer based on an input shape</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dense_units</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ratio</span><span class="p">))</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">pool_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling2D</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="n">pool_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling3D</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s1">&#39;SqueezeAndExcitation layer can only accept 4D or 5D data &#39;</span>
                <span class="s1">&#39;for image or video input but received input shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">input_shape</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">pool_layer</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dense_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Multiply</span><span class="p">(),</span>
        <span class="p">]</span></div>

<div class="viewcode-block" id="MaskedSqueezeAndExcitation.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.MaskedSqueezeAndExcitation.html#phygnn.layers.custom_layers.MaskedSqueezeAndExcitation.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call the custom SqueezeAndExcitation layer</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor.</span>
<span class="sd">        y : tf.Tensor</span>
<span class="sd">            Sparse input tensor used to mask ``x``</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor, this is the squeeze-and-excitation weights</span>
<span class="sd">            multiplied by the original input tensor x</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">t_in</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">is_nan</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">boolean_mask</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># multiply layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]([</span><span class="n">t_in</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="Attention"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.Attention.html#phygnn.layers.custom_layers.Attention">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">Attention</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Self-Attention block&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">key_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        num_heads : int</span>
<span class="sd">            Number of attention heads</span>
<span class="sd">        key_dim : int</span>
<span class="sd">            Size of each attention head</span>
<span class="sd">        name : str | None</span>
<span class="sd">            Name of layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key_dim</span> <span class="o">=</span> <span class="n">key_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">key_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">key_dim</span>
        <span class="p">)</span>

<div class="viewcode-block" id="Attention.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.Attention.html#phygnn.layers.custom_layers.Attention.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Self Attention block</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x_in</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">x_in</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span></div></div>


<div class="viewcode-block" id="AxialAttentionBlock"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.AxialAttentionBlock.html#phygnn.layers.custom_layers.AxialAttentionBlock">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">AxialAttentionBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Axial Self-Attention block</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    1. Axial Attention in Multidimensional Transformers.</span>
<span class="sd">       https://arxiv.org/abs/1912.12180</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">key_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        num_heads : int</span>
<span class="sd">            Number of attention heads</span>
<span class="sd">        key_dim : int</span>
<span class="sd">            Size of each attention head</span>
<span class="sd">        name : str | None</span>
<span class="sd">            Name of layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key_dim</span> <span class="o">=</span> <span class="n">key_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_layers</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="AxialAttentionBlock.build"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.AxialAttentionBlock.html#phygnn.layers.custom_layers.AxialAttentionBlock.build">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build the AxialAttentionBlock layer based on an input shape</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s1">&#39;AxialAttentionBlock must have at least 3 dimensions, but &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;received input shape: </span><span class="si">{</span><span class="n">input_shape</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">msg</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MultiHeadAttention</span><span class="p">(</span>
                <span class="n">num_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">key_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">key_dim</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
        <span class="p">]</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_attention_along_axis</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply attention along the given axis. Flattens all other</span>
<span class="sd">        spatiotemporal axes and then reshapes the attended output&quot;&quot;&quot;</span>

        <span class="c1"># Permute so the axis becomes the &quot;sequence&quot; dimension</span>
        <span class="n">order</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">))</span>
        <span class="n">order</span><span class="p">[</span><span class="n">axis</span><span class="p">],</span> <span class="n">order</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">order</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">order</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
        <span class="n">x_perm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="n">order</span><span class="p">)</span>

        <span class="c1"># Flatten the non-sequence dims and compute attention</span>
        <span class="n">x_flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_perm</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_perm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">x_perm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">x_attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_layers</span><span class="p">[</span><span class="n">axis</span> <span class="o">-</span> <span class="mi">1</span><span class="p">](</span><span class="n">x_flat</span><span class="p">,</span> <span class="n">x_flat</span><span class="p">)</span>
        <span class="n">x_attn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">()([</span><span class="n">x_flat</span><span class="p">,</span> <span class="n">x_attn</span><span class="p">])</span>  <span class="c1"># Residual</span>

        <span class="c1"># Reshape and permute back</span>
        <span class="n">x_attn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_attn</span><span class="p">,</span> <span class="n">x_perm</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">inv_permute</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">order</span><span class="p">)</span>
        <span class="n">x_attn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x_attn</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="n">inv_permute</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_attn</span>

<div class="viewcode-block" id="AxialAttentionBlock.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.AxialAttentionBlock.html#phygnn.layers.custom_layers.AxialAttentionBlock.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Applies attention along each spatial and the temporal axis</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># ida=0 is the batch axis and ida=-1 is the feature axis</span>
        <span class="k">for</span> <span class="n">ida</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_attention_along_axis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">ida</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="SparseAttention"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SparseAttention.html#phygnn.layers.custom_layers.SparseAttention">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">SparseAttention</span><span class="p">(</span><span class="n">Attention</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sparse Scan Self-Attention block</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Fan, Qihang, et al. &quot;Vision Transformer with Sparse Scan Prior.&quot; arXiv</span>
<span class="sd">    preprint arXiv:2405.13335 (2024).</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SparseAttention.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SparseAttention.html#phygnn.layers.custom_layers.SparseAttention.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call the Sparse Scan Self Attention block</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor.</span>
<span class="sd">        y : tf.Tensor</span>
<span class="sd">            Tensor with possible NaN values, used to create mask.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x_in</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">is_nan</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">x_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="p">(</span><span class="o">*</span><span class="n">x_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">x_in</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span></div></div>


<div class="viewcode-block" id="CBAM"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.CBAM.html#phygnn.layers.custom_layers.CBAM">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">CBAM</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convolutional Block Attention Module</span>

<span class="sd">    Note that this is only set up to take a channels-last conv output</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    1. Woo, Sanghyun, et al. &quot;Cbam: Convolutional block attention module.&quot;</span>
<span class="sd">       Proceedings of the European conference on computer vision (ECCV). 2018.</span>
<span class="sd">    2. Ma, Bing, et al. &quot;CBAM-GAN: generative adversarial networks based on</span>
<span class="sd">       convolutional block attention module.&quot; Artificial Intelligence and</span>
<span class="sd">       Security: 5th International Conference, ICAIS 2019, New York, NY, USA,</span>
<span class="sd">       July 26-28, 2019, Proceedings, Part I 5. Springer International</span>
<span class="sd">       Publishing, 2019.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ratio</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ratio : int</span>
<span class="sd">            Number of convolutional channels/filters divided by the number of</span>
<span class="sd">            dense connections in the CBAM block.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ratio</span> <span class="o">=</span> <span class="n">ratio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dense_units</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ch_avg</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ch_max</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ch_scale</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_st_scale</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="CBAM.build"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.CBAM.html#phygnn.layers.custom_layers.CBAM.build">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build the CBAM layer based on an input shape</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dense_units</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ratio</span><span class="p">))</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">avg_pool_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling2D</span><span class="p">()</span>
            <span class="n">max_pool_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalMaxPooling2D</span><span class="p">()</span>
            <span class="n">conv_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
                <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span>
            <span class="p">)</span>
            <span class="n">reshape_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="n">avg_pool_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling3D</span><span class="p">()</span>
            <span class="n">max_pool_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalMaxPooling3D</span><span class="p">()</span>
            <span class="n">conv_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv3D</span><span class="p">(</span>
                <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span>
            <span class="p">)</span>
            <span class="n">reshape_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">((</span>
                <span class="mi">1</span><span class="p">,</span>
                <span class="mi">1</span><span class="p">,</span>
                <span class="mi">1</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span><span class="p">,</span>
            <span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s1">&#39;CBAM layer can only accept 4D or 5D data for image or video &#39;</span>
                <span class="s1">&#39;input but received input shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_ch_avg</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">avg_pool_layer</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dense_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">),</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ch_max</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">max_pool_layer</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dense_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">),</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ch_scale</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">(),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">),</span>
            <span class="n">reshape_layer</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Multiply</span><span class="p">(),</span>
        <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_st_scale</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">conv_layer</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Multiply</span><span class="p">(),</span>
        <span class="p">]</span></div>

<div class="viewcode-block" id="CBAM.channel_attention"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.CBAM.html#phygnn.layers.custom_layers.CBAM.channel_attention">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">channel_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call the channel attention block</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor, this is the channel attention weights</span>
<span class="sd">            multiplied by the original input tensor x</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">t_in</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">avg_pool</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">max_pool</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ch_avg</span><span class="p">:</span>
            <span class="n">avg_pool</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">avg_pool</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ch_max</span><span class="p">:</span>
            <span class="n">max_pool</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">max_pool</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">avg_pool</span><span class="p">,</span> <span class="n">max_pool</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ch_scale</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># multiply layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ch_scale</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]([</span><span class="n">t_in</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">x</span></div>

<div class="viewcode-block" id="CBAM.spatiotemporal_attention"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.CBAM.html#phygnn.layers.custom_layers.CBAM.spatiotemporal_attention">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">spatiotemporal_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call the spatiotemporal attention block</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor, this is the spatiotemporal attention weights</span>
<span class="sd">            multiplied by the original input tensor x</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">t_in</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">avg_pool</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">max_pool</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">avg_pool</span><span class="p">,</span> <span class="n">max_pool</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_st_scale</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># multiply layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_st_scale</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]([</span><span class="n">t_in</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">x</span></div>

<div class="viewcode-block" id="CBAM.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.CBAM.html#phygnn.layers.custom_layers.CBAM.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call the full CBAM block</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor, this is channel attention followed by spatiotemporal</span>
<span class="sd">            attention</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">channel_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spatiotemporal_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="FNO"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.FNO.html#phygnn.layers.custom_layers.FNO">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">FNO</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Custom layer for fourier neural operator block</span>

<span class="sd">    Note that this is only set up to take a channels-last input</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    1. FourCastNet: A Global Data-driven High-resolution Weather Model using</span>
<span class="sd">    Adaptive Fourier Neural Operators. http://arxiv.org/abs/2202.11214</span>
<span class="sd">    2. Adaptive Fourier Neural Operators: Efficient Token Mixers for</span>
<span class="sd">    Transformers. http://arxiv.org/abs/2111.13587</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">sparsity_threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        filters : int</span>
<span class="sd">            Number of dense connections in the FNO block.</span>
<span class="sd">        sparsity_threshold : float</span>
<span class="sd">            Parameter to control sparsity and shrinkage in the softshrink</span>
<span class="sd">            activation function following the MLP layers.</span>
<span class="sd">        activation : str</span>
<span class="sd">            Activation function used in MLP layers.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_filters</span> <span class="o">=</span> <span class="n">filters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fft_layer</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ifft_layer</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_layers</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_perms_in</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_perms_out</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lambd</span> <span class="o">=</span> <span class="n">sparsity_threshold</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_softshrink</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Softshrink activation function</span>

<span class="sd">        https://pytorch.org/docs/stable/generated/torch.nn.Softshrink.html</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">values_below_lower</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_lambd</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lambd</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">values_above_upper</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_lambd</span> <span class="o">&lt;</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lambd</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">values_below_lower</span> <span class="o">+</span> <span class="n">values_above_upper</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_fft</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply needed transpositions and fft operation.&quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_perms_in</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fft_layer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex64</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_perms_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_ifft</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply needed transpositions and ifft operation.&quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_perms_in</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ifft_layer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex64</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_perms_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<div class="viewcode-block" id="FNO.build"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.FNO.html#phygnn.layers.custom_layers.FNO.build">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build the FNO layer based on an input shape</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">dims</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_perms_in</span> <span class="o">=</span> <span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">dims</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_perms_out</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fft_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">signal</span><span class="o">.</span><span class="n">fft2d</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ifft_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">signal</span><span class="o">.</span><span class="n">ifft2d</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fft_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">signal</span><span class="o">.</span><span class="n">fft3d</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ifft_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">signal</span><span class="o">.</span><span class="n">ifft3d</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s1">&#39;FNO layer can only accept 4D or 5D data for image or video &#39;</span>
                <span class="s1">&#39;input but received input shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_filters</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span><span class="p">),</span>
        <span class="p">]</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_mlp_block</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run mlp layers on input&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<div class="viewcode-block" id="FNO.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.FNO.html#phygnn.layers.custom_layers.FNO.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call the custom FourierNeuralOperator layer</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor, this is the FNO weights added to the original input</span>
<span class="sd">            tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">t_in</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fft</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_softshrink</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ifft</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">t_in</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">t_in</span></div></div>


<div class="viewcode-block" id="Sup3rAdder"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.Sup3rAdder.html#phygnn.layers.custom_layers.Sup3rAdder">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">Sup3rAdder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Layer to add high-resolution data to a sup3r model in the middle of a</span>
<span class="sd">    super resolution forward pass.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str | None</span>
<span class="sd">            Unique str identifier of the adder layer. Usually the name of the</span>
<span class="sd">            hi-resolution feature used in the addition.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

<div class="viewcode-block" id="Sup3rAdder.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.Sup3rAdder.html#phygnn.layers.custom_layers.Sup3rAdder.call">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">hi_res_adder</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Adds hi-resolution data to the input tensor x in the middle of a</span>
<span class="sd">        sup3r resolution network.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor</span>
<span class="sd">        hi_res_adder : tf.Tensor | np.ndarray</span>
<span class="sd">            This should be a 4D array for spatial enhancement model or 5D array</span>
<span class="sd">            for a spatiotemporal enhancement model (obs, spatial_1, spatial_2,</span>
<span class="sd">            (temporal), features) that can be added to x.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor with the hi_res_adder added to x.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">hi_res_adder</span></div></div>


<div class="viewcode-block" id="Sup3rConcatObs"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.Sup3rConcatObs.html#phygnn.layers.custom_layers.Sup3rConcatObs">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">Sup3rConcatObs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Layer to concatenate sparse data in the middle of a super resolution</span>
<span class="sd">    forward pass. This is used to condition models on sparse observation data.</span>
<span class="sd">    This uses the first channel of the input tensor as a background for the</span>
<span class="sd">    provided values and then concatenates with the input tensor.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fill_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">include_mask</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str | None</span>
<span class="sd">            Unique str identifier of the layer. Usually the name of the</span>
<span class="sd">            hi-resolution feature used in the concatenation.</span>
<span class="sd">        fill_method : str | None</span>
<span class="sd">            Method to use for filling the NaN values in the hi_res_feature.</span>
<span class="sd">            If this is None then the first channel of x will be used.</span>
<span class="sd">            Otherwise, accepted values are &#39;mean&#39; and &#39;idw&#39;.</span>
<span class="sd">        include_mask : bool</span>
<span class="sd">            If True, the mask of the hi_res_feature showing where there is</span>
<span class="sd">            valid observation data will be included in the concatenation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">fill_method</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fill_method</span> <span class="o">=</span> <span class="n">mean_fill</span>
        <span class="k">elif</span> <span class="n">fill_method</span> <span class="o">==</span> <span class="s1">&#39;idw&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fill_method</span> <span class="o">=</span> <span class="n">idw_fill</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fill_method</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">include_mask</span> <span class="o">=</span> <span class="n">include_mask</span>

<div class="viewcode-block" id="Sup3rConcatObs.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.Sup3rConcatObs.html#phygnn.layers.custom_layers.Sup3rConcatObs.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">hi_res_feature</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Combine the first channel of x and the non-nan data in</span>
<span class="sd">        hi_res_feature and concatenate with x.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor</span>
<span class="sd">        hi_res_feature : tf.Tensor | np.ndarray</span>
<span class="sd">            This should be a 4D array for spatial enhancement model or 5D array</span>
<span class="sd">            for a spatiotemporal enhancement model (obs, spatial_1, spatial_2,</span>
<span class="sd">            (temporal), 1). This is NaN where there are no observations and</span>
<span class="sd">            real values where observations exist.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor with the hi_res_feature used to fix values of x.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">hi_res_feature</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hi_res_feature</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fill_method</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">is_nan</span><span class="p">(</span><span class="n">hi_res_feature</span><span class="p">)</span>
            <span class="n">fixed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">hi_res_feature</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fixed</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fill_method</span><span class="p">(</span><span class="n">hi_res_feature</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_mask</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">fixed</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">fixed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">fixed</span><span class="p">,</span> <span class="n">mask</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">fixed</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Sup3rObsModel"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.Sup3rObsModel.html#phygnn.layers.custom_layers.Sup3rObsModel">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">Sup3rObsModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Layer to concatenate sparse data in the middle of a super</span>
<span class="sd">    resolution forward pass, with a learned embedding. Mutiple observation</span>
<span class="sd">    features and multiple continuous exogenous features can be provided.</span>
<span class="sd">    The embedding network is defined with a list of hidden layers. If no</span>
<span class="sd">    hidden layers are provided, this layer will simply concatenate the</span>
<span class="sd">    hi_res_feature, exogenous data (if provided), and mask (if</span>
<span class="sd">    ``include_mask`` is True), to the input tensor after filling the</span>
<span class="sd">    NaNs.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">exo_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hidden_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">fill_method</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span>
        <span class="n">include_mask</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str | None</span>
<span class="sd">            Unique str identifier of the layer. Usually the name of the</span>
<span class="sd">            hi-resolution feature used in the concatenation.</span>
<span class="sd">        features : list | None</span>
<span class="sd">            The names of the observation features to be included in the</span>
<span class="sd">            embedding input.</span>
<span class="sd">        exo_features : list | None</span>
<span class="sd">            The names of exogenous features to be included in the embedding</span>
<span class="sd">            input</span>
<span class="sd">        hidden_layers : list | None</span>
<span class="sd">            The list of layers used to create the embedding network.</span>
<span class="sd">        fill_method : str</span>
<span class="sd">            The method used to fill in the NaN values in the hi_res_feature</span>
<span class="sd">            before embedding. Options are &#39;mean&#39;, &#39;idw&#39;, or None. If None then</span>
<span class="sd">            the first channel of x will be used to fill the NaN values.</span>
<span class="sd">        include_mask : bool</span>
<span class="sd">            Whether to include the mask for where there is valid observation</span>
<span class="sd">            data in the embedding. If False, the mask will not be included in</span>
<span class="sd">            the embedding.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layers</span> <span class="o">=</span> <span class="n">hidden_layers</span> <span class="ow">or</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">features</span> <span class="ow">or</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exo_features</span> <span class="o">=</span> <span class="n">exo_features</span> <span class="ow">or</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">include_mask</span> <span class="o">=</span> <span class="n">include_mask</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fill_method</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">fill_method</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fill_method</span> <span class="o">=</span> <span class="n">mean_fill</span>
        <span class="k">elif</span> <span class="n">fill_method</span> <span class="o">==</span> <span class="s1">&#39;idw&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fill_method</span> <span class="o">=</span> <span class="n">idw_fill</span>

<div class="viewcode-block" id="Sup3rObsModel.build"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.Sup3rObsModel.html#phygnn.layers.custom_layers.Sup3rObsModel.build">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build the weight net layer based on an input shape</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="Sup3rObsModel.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.Sup3rObsModel.html#phygnn.layers.custom_layers.Sup3rObsModel.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">hi_res_feature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exo_data</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the embed net to hi_res_feature, exogenous data, and the</span>
<span class="sd">        mask representing where hi_res_feature is not nan. Concatenate the</span>
<span class="sd">        output with x. ``hi_res_feature`` and ``exo_data`` are allowed to be</span>
<span class="sd">        None so that models can be trained with hi_res_feature and exogenous</span>
<span class="sd">        data and then run with various sets of inputs.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor</span>
<span class="sd">        hi_res_feature : tf.Tensor | np.ndarray | None</span>
<span class="sd">            This should be a 4D array for spatial enhancement model or 5D array</span>
<span class="sd">            for a spatiotemporal enhancement model (obs, spatial_1, spatial_2,</span>
<span class="sd">            (temporal), features). This is NaN where there are no observations</span>
<span class="sd">            and real values where observations exist.</span>
<span class="sd">        exo_data : tf.Tensor | np.ndarray | None</span>
<span class="sd">            This is an array of exogenous data used to imform the embedding,</span>
<span class="sd">            like topography</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor with embedding concatenated to input.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">hi_res_feature</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hr_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">))</span>
            <span class="n">hi_res_feature</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">hr_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">exo_data</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">exo_features</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">exo_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">exo_features</span><span class="p">))</span>
            <span class="n">exo_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">exo_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fill_method</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">is_nan</span><span class="p">(</span><span class="n">hi_res_feature</span><span class="p">)</span>
            <span class="n">hr_feat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                <span class="n">mask</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">)],</span> <span class="n">hi_res_feature</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hr_feat</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fill_method</span><span class="p">(</span><span class="n">hi_res_feature</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_mask</span><span class="p">:</span>
            <span class="n">embed</span> <span class="o">=</span> <span class="n">hr_feat</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">hr_feat</span><span class="p">,</span> <span class="n">mask</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">exo_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">exo_data</span><span class="p">,</span> <span class="n">embed</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layers</span><span class="p">:</span>
            <span class="n">embed</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">embed</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">embed</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Sup3rConcat"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.Sup3rConcat.html#phygnn.layers.custom_layers.Sup3rConcat">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">Sup3rConcat</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Layer to concatenate a high-resolution feature to a sup3r model in the</span>
<span class="sd">    middle of a super resolution forward pass.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str | None</span>
<span class="sd">            Unique str identifier for the concat layer. Usually the name of the</span>
<span class="sd">            hi-resolution feature used in the concatenation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

<div class="viewcode-block" id="Sup3rConcat.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.Sup3rConcat.html#phygnn.layers.custom_layers.Sup3rConcat.call">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">hi_res_feature</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Concatenates a hi-resolution feature to the input tensor x in the</span>
<span class="sd">        middle of a sup3r resolution network.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor</span>
<span class="sd">        hi_res_feature : tf.Tensor | np.ndarray</span>
<span class="sd">            This should be a 4D array for spatial enhancement model or 5D array</span>
<span class="sd">            for a spatiotemporal enhancement model (obs, spatial_1, spatial_2,</span>
<span class="sd">            (temporal), features) that can be concatenated to x.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor with the hi_res_feature added to x.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">hi_res_feature</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="FunctionalLayer"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.FunctionalLayer.html#phygnn.layers.custom_layers.FunctionalLayer">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">FunctionalLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Custom layer to implement the tensorflow layer functions (e.g., add,</span>
<span class="sd">    subtract, multiply, maximum, and minimum) with a constant value. These</span>
<span class="sd">    cannot be implemented in phygnn as normal layers because they need to</span>
<span class="sd">    operate on two tensors of equal shape.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str</span>
<span class="sd">            Name of the tensorflow layer function to be implemented, options</span>
<span class="sd">            are (all lower-case): add, subtract, multiply, maximum, and minimum</span>
<span class="sd">        value : float</span>
<span class="sd">            Constant value to use in the function operation</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">options</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">,</span> <span class="s1">&#39;subtract&#39;</span><span class="p">,</span> <span class="s1">&#39;multiply&#39;</span><span class="p">,</span> <span class="s1">&#39;maximum&#39;</span><span class="p">,</span> <span class="s1">&#39;minimum&#39;</span><span class="p">)</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;FunctionalLayer input `name` must be one of &quot;</span><span class="si">{</span><span class="n">options</span><span class="si">}</span><span class="s1">&quot; &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;but received &quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">&quot;&#39;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">options</span><span class="p">,</span> <span class="n">msg</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fun</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

<div class="viewcode-block" id="FunctionalLayer.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.FunctionalLayer.html#phygnn.layers.custom_layers.FunctionalLayer.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Operates on x with the specified function</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor operated on by the specified function</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">const</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fun</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">const</span><span class="p">))</span></div></div>


<div class="viewcode-block" id="SigLin"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SigLin.html#phygnn.layers.custom_layers.SigLin">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">SigLin</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sigmoid linear unit. This can be used to set a soft minimum on a range.</span>

<span class="sd">    y = 1/(1+exp(-x)) where x&lt;0.5</span>
<span class="sd">    y = x + 0.5 where x&gt;=0.5</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SigLin.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.SigLin.html#phygnn.layers.custom_layers.SigLin.call">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Operates on x with SigLin</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Output tensor with same shape as input x operated on by SigLin</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="LogTransform"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.LogTransform.html#phygnn.layers.custom_layers.LogTransform">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">LogTransform</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Log transform or inverse transform of data</span>

<span class="sd">    ``y = log(x + adder) * scalar`` or</span>
<span class="sd">    ``y = exp(x / scalar) - adder`` for the inverse</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">adder</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scalar</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">idf</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str | None</span>
<span class="sd">            Name of the tensorflow layer</span>
<span class="sd">        adder : float</span>
<span class="sd">            Adder term for ``y = log(x + adder) * scalar``</span>
<span class="sd">        scalar : float</span>
<span class="sd">            Scalar term for ``y = log(x + adder) * scalar``</span>
<span class="sd">        inverse : bool</span>
<span class="sd">            Option to perform the inverse operation e.g.</span>
<span class="sd">            ``y = exp(x / scalar) - adder``</span>
<span class="sd">        idf : int | list | None</span>
<span class="sd">            One or more feature channel indices to perform log transform on.</span>
<span class="sd">            None will perform transform on all feature channels.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adder</span> <span class="o">=</span> <span class="n">adder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scalar</span> <span class="o">=</span> <span class="n">scalar</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inverse</span> <span class="o">=</span> <span class="n">inverse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idf</span> <span class="o">=</span> <span class="p">[</span><span class="n">idf</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">idf</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">idf</span>

<div class="viewcode-block" id="LogTransform.build"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.LogTransform.html#phygnn.layers.custom_layers.LogTransform.build">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Custom implementation of the tf layer build method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_logt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">inverse</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">adder</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scalar</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">scalar</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">adder</span>

<div class="viewcode-block" id="LogTransform.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.LogTransform.html#phygnn.layers.custom_layers.LogTransform.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Operates on x with (inverse) log transform</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : tf.Tensor</span>
<span class="sd">            Log-transformed x tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">idf</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idf</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
            <span class="k">if</span> <span class="n">idf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">idf</span><span class="p">:</span>
                <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_logt</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">idf</span><span class="p">:</span> <span class="n">idf</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">idf</span><span class="p">:</span> <span class="n">idf</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;concat&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span></div></div>


<div class="viewcode-block" id="UnitConversion"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.UnitConversion.html#phygnn.layers.custom_layers.UnitConversion">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">UnitConversion</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Layer to convert units per feature channel using the linear transform:</span>
<span class="sd">    ``y = x * scalar + adder``</span>

<span class="sd">    Be sure to check how this will interact with normalization factors.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">adder</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scalar</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str | None</span>
<span class="sd">            Name of the tensorflow layer</span>
<span class="sd">        adder : float | list</span>
<span class="sd">            Adder term for ``y = x * scalar + adder``. If this is a float, the</span>
<span class="sd">            same value will be used for all feature channels. If this is a</span>
<span class="sd">            list, each value will be used for the corresponding feature channel</span>
<span class="sd">            and the length must match the number of feature channels</span>
<span class="sd">        scalar : float | list</span>
<span class="sd">            Scalar term for ``y = x * scalar + adder``. If this is a float, the</span>
<span class="sd">            same value will be used for all feature channels. If this is a</span>
<span class="sd">            list, each value will be used for the corresponding feature channel</span>
<span class="sd">            and the length must match the number of feature channels</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adder</span> <span class="o">=</span> <span class="n">adder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scalar</span> <span class="o">=</span> <span class="n">scalar</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="UnitConversion.build"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.UnitConversion.html#phygnn.layers.custom_layers.UnitConversion.build">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Custom implementation of the tf layer build method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_shape : tuple</span>
<span class="sd">            Shape tuple of the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="n">nfeat</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">dtypes</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adder</span><span class="p">,</span> <span class="n">dtypes</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">adder</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nfeat</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">adder</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">adder</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adder</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;UnitConversion layer `adder` array has length &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adder</span><span class="p">)</span><span class="si">}</span><span class="s1"> but input shape has last dimension &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;as </span><span class="si">{</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adder</span><span class="p">)</span> <span class="o">==</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">msg</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scalar</span><span class="p">,</span> <span class="n">dtypes</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scalar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nfeat</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scalar</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scalar</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scalar</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;UnitConversion layer `scalar` array has length &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scalar</span><span class="p">)</span><span class="si">}</span><span class="s1"> but input shape has last dimension &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;as </span><span class="si">{</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scalar</span><span class="p">)</span> <span class="o">==</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">msg</span></div>

<div class="viewcode-block" id="UnitConversion.call"><a class="viewcode-back" href="../../../_autosummary/phygnn.layers.custom_layers.UnitConversion.html#phygnn.layers.custom_layers.UnitConversion.call">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert units</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : tf.Tensor</span>
<span class="sd">            Input tensor</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : tf.Tensor</span>
<span class="sd">            Unit-converted x tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idf</span><span class="p">,</span> <span class="p">(</span><span class="n">adder</span><span class="p">,</span> <span class="n">scalar</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">adder</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scalar</span><span class="p">)):</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">idf</span><span class="p">:</span> <span class="n">idf</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">scalar</span> <span class="o">+</span> <span class="n">adder</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;concat&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span></div></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Alliance for Sustainable Energy, LLC.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>